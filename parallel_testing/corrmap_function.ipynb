{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what I have done/learnt/know are the new problems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import maximum_filter, gaussian_filter, uniform_filter\n",
    "import time \n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import dask as da\n",
    "import dask_image.ndfilters\n",
    "import zarr\n",
    "\n",
    "#Params\n",
    "hpf_width = 100 # order of 100\n",
    "sdnorm_exp = 1 # range 0.8-1\n",
    "npil_filt = gaussian_filter\n",
    "npil_size = (1.0, 5.0, 5.0)\n",
    "cv_filt = uniform_filter\n",
    "cv_size = (1, 2, 2) # in dask needs to be int\n",
    "int_thresh = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and commented basic function, This is the non-parallelised but working  function\n",
    "\n",
    "#intput data is 4-D array(t,z,y,x)\n",
    "\n",
    "def minimal_corrmap_base(movx,):\n",
    "    \"\"\"Input a 4-D array (t,z,y,x) \"\"\"\n",
    "    nt,nz,ny,nx = movx.shape\n",
    "\n",
    "    #basic highpass filter\n",
    "    mov_hpf = np.zeros_like(movx)\n",
    "    for i in range(0, nt, hpf_width):\n",
    "        mov_hpf[i:i+hpf_width] = movx[i:i+hpf_width] - movx[i:i+hpf_width].mean(axis=0) # subtracts mean of a width\n",
    "\n",
    "    del movx\n",
    "    sdmov = (((np.diff(mov_hpf, axis=0) ** 2).sum(axis=0)) / nt)**0.5 #standard deviation of each array point through time\n",
    "    sdmov = np.greater(10 ** -9, sdmov)\n",
    "    norm_mov = mov_hpf / (sdmov ** sdnorm_exp) #normalised the HPF movie\n",
    "    del mov_hpf\n",
    "    \n",
    "\n",
    "    c1 = npil_filt(np.ones((nz,ny,nx)), sigma = npil_size, mode='constant')\n",
    "    #c2 = cv_filt(np.ones((nz,ny,nx)), cv_size, mode='constant')\n",
    "\n",
    "    #each time step apply the neuropil filter\n",
    "    np_mov = np.zeros_like(norm_mov) \n",
    "    for i in range(len(norm_mov[0])):\n",
    "        np_mov[i] = npil_filt(norm_mov[i], sigma = npil_size, mode='constant') / c1\n",
    "\n",
    "\n",
    "    np_sub_mov = norm_mov - np_mov # removes the neuropil from the normalised image\n",
    "    del norm_mov\n",
    "    del np_mov\n",
    "\n",
    "    \n",
    "    #applies a cell detection filter\n",
    "    cv_mov = np.zeros_like(np_sub_mov)\n",
    "    for i in range(len(np_sub_mov[0])):\n",
    "        cv_mov[i] = cv_filt(np_sub_mov[i], cv_size, mode='constant') * cv_size[-1] #should this be c2\n",
    "\n",
    "    #removes values below a threshold defult = 0, and sum over time\n",
    "    vmap = ((cv_mov ** 2) * (cv_mov > int_thresh).astype(int)).sum(axis=0) ** 0.5\n",
    "\n",
    "    return vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### There may be away to explicity store parts to disk or use .persist cleverly to eliminate the disk spill, personally I dont knowhow without spening more time learning about dask\n",
    "### The cuurent problem seems to be down to I/O, and is exasterbated if input array is big (~ a sizebale fraction of the RAM (1/4 ~1/2))\n",
    "### The problem maybe that one(+) of the functions aren't lazy, and them being called is a slow down\n",
    "\n",
    "\n",
    "def minimal_corrmap_dask(movx):\n",
    "    \"\"\"Input 4-D dask array form (t,z,y,x)\"\"\"\n",
    "    nt,nz,ny,nx = movx.shape\n",
    "\n",
    "    # Here is naive rechunk based of the number of cores I have and to chunk so chunks are [100MB-1GB]\n",
    "    # spatial_chunks = (-1,nz, ny//12, nx//12)\n",
    "    # time_chunks = (10,nz,ny,nx)\n",
    "    # here was ~ the optimal for my set up and data set (100,30,800,500)\n",
    "    spatial_chunks = (-1,nz,200,150)\n",
    "    time_chunks = (5,nz,ny,nx)\n",
    "    \n",
    "\n",
    "    #basic highpass filter\n",
    "    movx.rechunk(spatial_chunks)\n",
    "    mov_hpf = da.array.zeros_like(movx)\n",
    "    for i in range(0, nt, hpf_width):\n",
    "        mov_hpf[i:i+hpf_width] = movx[i:i+hpf_width] - movx[i:i+hpf_width].mean(axis=0) # subtracts mean of a width\n",
    "\n",
    "    sdmov = (((da.array.diff(mov_hpf, axis=0) ** 2).sum(axis=0)) / nt)**0.5  #standard deviation of each array point through time\n",
    "    sdmov = da.array.maximum(10 ** -9,sdmov) \n",
    "\n",
    "    norm_mov = mov_hpf / (sdmov ** sdnorm_exp) #normalised the HPF movie .. probably current error\n",
    "\n",
    "    del mov_hpf\n",
    "    del sdmov \n",
    "\n",
    "    norm_mov.rechunk(time_chunks)\n",
    "\n",
    "    c1 = dask_image.ndfilters.gaussian_filter(da.array.ones((nz,ny,nx)), sigma = npil_size, mode='constant')\n",
    "    #c2 = dask_image.ndfilters.uniform_filter(da.array.ones((nz,ny,nx)), cv_size, mode='constant')\n",
    "\n",
    "    #each time step apply the neuropil filter\n",
    "    np_mov = da.array.zeros_like(movx)\n",
    "    for i in range(len(movx[0])):\n",
    "        np_mov[i] = dask_image.ndfilters.gaussian_filter(norm_mov[i], sigma = npil_size, mode='constant') / c1\n",
    "\n",
    "\n",
    "    np_sub_mov = norm_mov - np_mov # removes the neuropil from the normalised image\n",
    "\n",
    "    del np_mov\n",
    "    del norm_mov\n",
    "\n",
    "    #applies a cell detection filter\n",
    "    cv_mov = da.array.zeros_like(movx)\n",
    "    for i in range(len(movx[0])):\n",
    "        cv_mov[i] = dask_image.ndfilters.uniform_filter(np_sub_mov[i], cv_size, mode='constant') * cv_size[-1]\n",
    "\n",
    "    del np_sub_mov\n",
    "\n",
    "    #removes values below a threshold defult = 0, and sum over time\n",
    "    cv_mov.rechunk(spatial_chunks)\n",
    "    vmap = (((cv_mov ** 2) * (cv_mov > int_thresh).astype(int)).sum(axis=0) ** 0.5)\n",
    "\n",
    "    return vmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been stroing and loading as zarr e.g data_dask = da.array.from_zarr('test_data_larger.zarr')\n",
    "\n",
    "test_data_larger, is sliced in  x,y (1256,972) -> (800,500)  to make it managable on my 16GB of RAM\n",
    "\n",
    "On this data the base function ~ 196 seconds, 145 seconds (ran 2 times)\n",
    "\n",
    "and the dask function ~ 37 seconds, 36 seconds (ran 2 times )\n",
    "\n",
    "If data of the input is a few time less than RAM, the spepd is significantly quicker as spilling to the disk is the bottle neck, (These following numbers arnt 100% accurate just to give an idea) The input data for the (100,30,1256,972) is ~ 11GB as zarr, therfore RAM of ~> 40GB should be able to run it without spilling, therfore much quicker.\n",
    "\n",
    "The dask function *should* be able to generalise to larger than ram data sets however this will bring significant slowdown from spilling to disk and sometimes has not been able to run.\n",
    "There may be some memory leak, i havent gotten around to testing it/learning how to, however I belive there are other process on my laptop which could influence this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
