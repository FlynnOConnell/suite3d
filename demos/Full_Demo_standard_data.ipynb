{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57652f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import numpy as n\n",
    "from matplotlib import pyplot as plt\n",
    "import napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c9b786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install gitpython for dev benchmarking to work\n"
     ]
    }
   ],
   "source": [
    "# path to wherever the s2p-lbm repository is cloned on your computer\n",
    "os.chdir(os.path.dirname(os.path.abspath(\"\")))\n",
    "\n",
    "# set to \"false\" to run code without messages intended for developers\n",
    "os.environ[\"SUITE3D_DEVELOPER\"] = \"true\"\n",
    "\n",
    "from suite3d.job import Job\n",
    "from suite3d import ui\n",
    "from suite3d import io\n",
    "from suite3d import tiff_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0d542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find all the tiffiles in the tif path\n",
    "# File I/O is pipelined, so the data doesn't have to be on a fast SSD \n",
    "# single HDDs or reasonably fast network drives should work without much difference in speed \n",
    "tif_path = r'\\\\zaru.cortexlab.net\\Subjects\\ATL020\\2023-04-12\\701' #C:\\Users\\andrew\\Documents\\localData\\suite3d\\Suite3D Demo Data' # r'/mnt/md0/data/demo'\n",
    "tifs = io.get_tif_paths(tif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b3e38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the mandatory parameters\n",
    "planes = n.arange(5)\n",
    "params = {\n",
    "    # volume rate\n",
    "    'fs': io.get_vol_rate(tifs[0]),\n",
    "    # planes to analyze. 0 is deepest, 30 is shallowest (corrected for ScanImage channel IDs)\n",
    "    # you should keep all the planes to do crosstalk estimation! \n",
    "    'planes' : planes,\n",
    "    'crosstalk_n_planes' : len(planes)//2,\n",
    "    \n",
    "    # Decay time of the Ca indicator in seconds. 1.3 for GCaMP6s. This example is for GCamP8m\n",
    "    'tau' : 1.3,\n",
    "    'lbm' : False, \n",
    "    'num_colors' : 2, # if not lbm data, how many color channels were recorded by scanimage\n",
    "    'functional_color_channel' : 0, # if not lbm data, which color channel is the functional one\n",
    "    'fuse_strips' : False, # don't do this, it's only needed for LBM data\n",
    "    'fix_shallow_plane_shift_estimates' : False,\n",
    "    'subtract_crosstalk' : False, # I think this is unnecessary for non LBM data...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506479ef-1b9f-40a9-8cf4-703efb27c443",
   "metadata": {},
   "source": [
    "### \"Job\" structure\n",
    "The unimaginatively named `Job` structure is meant to contain all of the parameters, data, logs, and results for a single recording. It will be created in the root directory provided with the given name. All intermediate and final results will be saved in this directory, so I recommend using a fast SSD for this (and moving results to slow HDD once processing is complete).\n",
    "\n",
    "All the print statements you see (and more) are also logged in `<job dir>/logfile.txt`. If you want things to look cleaner, reduce the verbosity to 2 (full logs will still be in the logfile).\n",
    "\n",
    "To load a previously created job (to do more processing or load results), set `create=False`. If `create=True` but there exists another job of the same name in the root directory, it will either overwrite the parameters of the previous job or throw an error (depending on the `overwrite` parameter). Note, overwriting isn't as catastrophic as it sounds since data isn't deleted and remains accessible, but you might lose the saved parameters and some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e02910a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job directory C:\\Users\\andrew\\Documents\\localData\\suite3d\\runs\\s3d-atl-test already exists\n",
      "Loading job directory for atl-test in C:\\Users\\andrew\\Documents\\localData\\suite3d\\runs\n",
      "   Loading dirs \n",
      "      Created dir C:\\Users\\andrew\\Documents\\localData\\suite3d\\runs\\s3d-atl-test\\registered_fused_data\n",
      "      Found dir summary\n",
      "      Created dir C:\\Users\\andrew\\Documents\\localData\\suite3d\\runs\\s3d-atl-test\\iters\n",
      "   Loading default params\n",
      "      Updating param fs\n",
      "      Updating param planes\n",
      "      Updating param crosstalk_n_planes\n",
      "      Updating param tau\n",
      "      Updating param lbm\n",
      "      Updating param num_colors\n",
      "      Updating param functional_color_channel\n",
      "      Updating param fuse_strips\n",
      "      Updating param fix_shallow_plane_shift_estimates\n",
      "      Updating param subtract_crosstalk\n",
      "   Updated main params file\n"
     ]
    }
   ],
   "source": [
    "# Create the job\n",
    "job = Job(r'C:\\Users\\andrew\\Documents\\localData\\suite3d\\runs','atl-test', tifs = tifs,\n",
    "          params=params, create=True, overwrite=True, verbosity = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61b005",
   "metadata": {},
   "source": [
    "## Initial pass\n",
    "This pass takes a few files (`n_init_files`, usually ~200-300 frames is enough) and does the following:\n",
    "- estimates the crosstalk coefficient between the lower set of 15 planes and the higher 15 planes\n",
    "- computes the shifts between successive planes caused by the xy-shift of the light beads\n",
    "- estimates the optimal number of pixels that overlap between successive strips, so they can be fused together\n",
    "- calculates a \"reference volume\" that will be used later in registration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22a29aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional parameters for initialization\n",
    "# load 1 file to initialize\n",
    "job.params['n_init_files'] = 1\n",
    "# If set to None, use all of the frames in the loaded init files \n",
    "# if your files are really big, set this to <300\n",
    "job.params['init_n_frames'] = None\n",
    "\n",
    "# Set to None to auto-compute the crosstalk coefficient\n",
    "# You can set a float value between 0-1 (usually around 0.1-0.3) to override the calculation\n",
    "job.params['override_crosstalk'] = None\n",
    "# number of processors to use\n",
    "job.params['n_proc_corr'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5310b6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saved a copy of params at C:\\Users\\andrew\\Documents\\localData\\suite3d\\runs\\s3d-atl-test\\summary\n",
      "   Updated main params file\n",
      "Launching initial pass\n",
      "Saving summary to C:\\Users\\andrew\\Documents\\localData\\suite3d\\runs\\s3d-atl-test\\summary\\summary.npy\n",
      "   Loading init tifs with 30 channels\n",
      "      Loading \\\\zaru.cortexlab.net\\Subjects\\ATL020\\2023-04-12\\701\\2023-04-12_701_ATL020_2P_00001_00009.tif\n",
      "   Loaded 1 files, total 0.98 GB\n",
      "   Loaded init tifs\n",
      "   Loaded movie with 400 frames and shape 5, 512, 512\n",
      "      Enforcing positivity in mean image\n",
      "   No crosstalk estimation or subtraction\n",
      "   Using 3d registration\n",
      "   Computing plane alignment shifts\n",
      "   Applying plane alignment shifts\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot assign slice from input of different size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:3\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\GitHub\\s2p-lbm\\suite3d\\job.py:365\u001b[0m, in \u001b[0;36mJob.run_init_pass\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_params(copy_dir_tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching initial pass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 365\u001b[0m \u001b[43minit_pass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_init_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\GitHub\\s2p-lbm\\suite3d\\init_pass.py:154\u001b[0m, in \u001b[0;36mrun_init_pass\u001b[1;34m(job)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reference_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3d_reg\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    152\u001b[0m     job\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing 3d registration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    153\u001b[0m     tvecs, ref_image, all_refs_and_masks, pad_sizes, reference_params, reference_info, shifted_mov \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m--> 154\u001b[0m     \u001b[43mref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_reference_and_masks_3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmov_fuse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_GPU\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_GPU_registration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     summary_mov_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(job\u001b[38;5;241m.\u001b[39mdirs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_mov.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    157\u001b[0m     n\u001b[38;5;241m.\u001b[39msave(summary_mov_path, shifted_mov)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\GitHub\\s2p-lbm\\suite3d\\reference_image.py:1061\u001b[0m, in \u001b[0;36mcompute_reference_and_masks_3d\u001b[1;34m(mov_cpu, reference_params, log_cb, rmins, rmaxs, use_GPU)\u001b[0m\n\u001b[0;32m   1057\u001b[0m log_cb(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying plane alignment shifts\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;66;03m# print(mov_cpu.shape)\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# print(tvecs.shape)\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# print(tvecs)\u001b[39;00m\n\u001b[1;32m-> 1061\u001b[0m mov_cpu \u001b[38;5;241m=\u001b[39m \u001b[43mreg_3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshift_mov_lbm_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmov_cpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtvecs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# apply the lbm shift\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_GPU:\n\u001b[0;32m   1064\u001b[0m     log_cb(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching 3D GPU reference image calculation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot assign slice from input of different size"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This step only uses `n_init_files` files, so the  runtime will stay the same even with larger recordings\n",
    "# soon this will also be gpu-ified to be faster!\n",
    "job.run_init_pass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656be88",
   "metadata": {},
   "source": [
    "## Registration\n",
    "First, we do registration over time of the  xy-drift caused by brain movement. This is similar to Suite2P registratrion, it does rigid registration followed by non-rigid registration. This is accelerated on the GPU. Suite2P registration parameters can be changed, see `default_params.py` for a list of all parameters related to registration. After you have registered, you can load the registered fused movie into memory and take a look at the mean image. I suggest cropping the dark edges if you have any as shown in the cells below.\n",
    "\n",
    "If you run out of gpu memory, try reducing the `gpu_reg_batchsize` parameter. I have a A4500 with 20GB memory which works well with a batchsize of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have large tiffs, split the large tiffs into files of size 100 after registration\n",
    "job.params['split_tif_size'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "job.register_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06fb0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GPU fails, the following *should* work\n",
    "# note that in the GPU version fusing is incorporated into registration\n",
    "# job.register()\n",
    "# job.params['n_skip'] = job.load_summary()['fuse_shift']\n",
    "# job.fuse_registered_movie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_full = job.get_registered_movie('registered_fused_data','f')\n",
    "im_full = mov_full.mean(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find crop values that minimize dark zones - check planes 0 and 15 in the following cell to \n",
    "# make sure you're not cutting out parts of the brain\n",
    "crop = ((0,18), (100,1100), (50, 900))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe27805",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.show_tif(im_full[0,crop[1][0]:crop[1][1], crop[2][0]:crop[2][1]])\n",
    "io.show_tif(im_full[len(job.params['planes'])//2,crop[1][0]:crop[1][1], crop[2][0]:crop[2][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cd4c60",
   "metadata": {},
   "source": [
    "## SVD Denoising\n",
    "We compute an SVD of the volumetric movie, and keep the top N components to denoise. This is done by blocking the volume to make it computationally tractable. The blocks have overlaps, and I find that for noisy movies you will get some grid-like artifacts if your block overlaps aren't set such that each non-edge pixel is included in at least two blocks. So, I usually set the overlaps to be half of the block shape to achieve this. Feel free to try with smaller (or zero) overlaps on your data to see if it works better (overlaps increase the number of blocks that need to be SVD-d, so less overlap = less blocks = faster compute). \n",
    "\n",
    "The SVD decomposition is implemented with Dask, which can be blazing fast (compared to other methods) if implemented correctly, but there are a few parameters that can make it really slow if set incorrectly. If the SVD feels slow, try playing with the `svd_pix_chunk` and `svd_time_chunk` parameters. If you really care about speed, probably install the Dask profiler and see if there are any obvious bottlenecks.\n",
    "\n",
    "Note that the Dask SVD uses an approximate algorithm and its runtime scales **sub-linearly** with movie length! So for a short movie, it might take >10x the movie duration, but for longer movies it should be much less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd086636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag (not path) for the directory where the SVD will be saved\n",
    "svd_dir_tag = 'svd_cropped'\n",
    "\n",
    "# Number of components to compute per block \n",
    "# (you can change the actual number of components used for computation later, this is the upper limit)\n",
    "job.params['n_svd_comp'] = 100\n",
    "# Size of each block in pixels in z,y,x\n",
    "job.params['svd_block_shape'] = (4,200,200)\n",
    "# overlap in z,y,x between two neighboring blocks\n",
    "job.params['svd_block_overlaps'] = (2,100,100)\n",
    "# crop the movie before computing svd\n",
    "job.params['svd_crop'] = crop\n",
    "\n",
    "# Number of pixels in each Dask \"chunk\" when computing SVD. Unless you have ridiculously \n",
    "# large blocks, manually setting the chunksize to the total number of pixels in a block\n",
    "# seems to be substantially faster than having multiple chunks per block\n",
    "job.params['svd_pix_chunk'] = n.product(job.params['svd_block_shape'])\n",
    "# When computing SVD, we can compute multiple blocks (4-8) at the same time, which is sometimes \n",
    "# faster since we save on some disk I/O for neighboring blocks (I  think)\n",
    "# for longer recordings (1000+frames) or if you have issues with RAM, set to 1\n",
    "job.params['n_svd_blocks_per_batch'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe618e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with_svd = False\n",
    "if with_svd:\n",
    "    # create the directory where we'll save the SVD, and run the SVD decomposition\n",
    "    job.make_new_dir(svd_dir_tag)\n",
    "    svd_info = job.svd_decompose_movie(svd_dir_tag, run_svd=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cdffb1",
   "metadata": {},
   "source": [
    "## Calculating the correlation map\n",
    "\n",
    "The correlation map is the most important part of the cell detection process. It spatially and temporally filters the denoised movie, normalizes it, thresholds it, and accumulates it over time to create a volume where cells should be made more visible and neuropil is removed.\n",
    "\n",
    "**You should tune some of these parameters for your data**, each described below. To enable easy tuning, there is a **parameter sweep interface** that can try many combinations for a subset of the movie quickly, and visualize the results (you will find this below). \n",
    "\n",
    "Correlation map improves the more frames you have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of SVD components to use when calculating the correlation map\n",
    "# lower number means more denoising, but if it's too low you will start losing cells!\n",
    "# This can't be larger than the number of svd components you used in the decomposition above\n",
    "job.params['n_svd_comp'] = 50\n",
    "\n",
    "# spatial filter sizes for neuropil subtraction, and cell detection\n",
    "# npil_filt is a low_pass filter that attempts to remove any features larger than the filter size (neuropil!)\n",
    "# conv_filt_xy is a high_pass filter that amplifies any features that are smaller than ~2x the filter size (cells!)\n",
    "# these values worked well for me with ~4um xy pixel spacing and ~15 um z pixel spacing, for detecting mouse somata\n",
    "# When you change resolution, or if you're trying to detect smaller things, you will need to adjust these values\n",
    "# because the units here are _pixels_, not microns!\n",
    "job.params['cell_filt_type'] = 'gaussian'\n",
    "job.params['cell_filt_xy_um'] = 9.0\n",
    "job.params['cell_filt_z_um'] = 0.6\n",
    "job.params['npil_filt_type'] = 'unif'\n",
    "job.params['npil_filt_xy'] = 25.0\n",
    "job.params['npil_filt_z']=  2.5\n",
    "\n",
    "# normalization exponent, should be around 1. \n",
    "# If you find blood vessels or the background being too bright in the correlation map, reduce it to ~0.7-0.8! \n",
    "job.params['sdnorm_exp']= 0.75\n",
    "\n",
    "# threshold applied to the normalized, filtered movie before it is accumulated into the correlation map\n",
    "# if you increase it, the background will become darker (which is good!), however at some point you will\n",
    "# start excluding dimmer cells (which is bad!)\n",
    "job.params['intensity_thresh']=0.3\n",
    "\n",
    "## Compute parameters \n",
    "# number of frames to compute at one iteration \n",
    "# (any value above ~100-200 shouldn't affect results, \n",
    "# decrease if you have RAM issues or if SVD reconstruction gets stuck on \"Sending all blocks to dask to compute\")\n",
    "job.params['t_batch_size'] = 300\n",
    "# number of processors to use when calculating the correlation map\n",
    "job.params['n_proc_corr'] = 12\n",
    "# number of frames per smaller batch within the batch, should be ~t_batch_size / n_proc_corr, but above ~5\n",
    "job.params['mproc_batchsize'] = 5\n",
    "\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "assert job.params['n_proc_corr'] < num_cores, f\"Your computer has {num_cores} but job.params['n_proc_corr'] is set to {job.params['n_proc_corr']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248bd9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# uncomment below to load svd_info for and svd you did earlier if you are re-running this notebook\n",
    "# svd_info = n.load(os.path.join(job.dirs['svd_cropped'], 'svd_info.npy'), allow_pickle=True).item()\n",
    "if with_svd:\n",
    "    corrmap = job.calculate_corr_map(mov = svd_info)\n",
    "else:\n",
    "    mov_full = job.get_registered_movie('registered_fused_data', 'fused')\n",
    "    crop = ((0,18), (100,1100), (50, 900))\n",
    "    mov_crop = mov_full[crop[0][0]:crop[0][1], :, crop[1][0]:crop[1][1], crop[2][0]:crop[2][1]]\n",
    "    corrmap = job.calculate_corr_map(mov = mov_crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ee735",
   "metadata": {},
   "source": [
    "### Optional: sweep correlation map parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e32d7-a4e5-4404-8dfc-07ed3a657ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the parameters you want to sweep, and enter them in the tuples.\n",
    "# It will do all combinations of parameters, so if you have many parameters it will be many combinations! \n",
    "\n",
    "do_sweep = True\n",
    "if do_sweep:\n",
    "    job.params['t_batch_size'] = 300\n",
    "    params_to_sweep = {\n",
    "        'intensity_thresh' : (0.3,),\n",
    "        #'n_svd_comp' : (50,), # if you have multiple values here, make sure you pass mov=svd_info\n",
    "        'cell_filt_xy_um': (2.0, 8.0, 16.0),\n",
    "        'npil_filt_xy_um': (5.0, 15.0, 25.0), # (15.0, 20.0, 25.0),\n",
    "        'cell_filt_z_um' : (0.6,),\n",
    "        'npil_filt_z_um' : (1.5, ), #(2.0, 3.0),\n",
    "        'sdnorm_exp' : (0.75,)\n",
    "    }\n",
    "    \n",
    "    sweep_summary = job.sweep_corrmap(params_to_sweep, mov = mov_full, all_combinations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b5247f-5042-4433-9719-d889f0d5325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to access older sweep results\n",
    "load_old_sweep = False        \n",
    "if do_sweep and load_old_sweep:\n",
    "    sweep_summary = n.load(os.path.join(job.dirs['sweep-full'], 'sweep_summary.npy'),allow_pickle=True).item()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05c8e3-ca82-400b-b1e0-a8832fd91d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to look at the sweep results\n",
    "if do_sweep:\n",
    "    v = job.vis_vmap_sweep(sweep_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105f811-bac2-4330-a05f-624eaa9e889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder: you should set all parameters that you swept back to the values you want explicitly before re-computing the correlation map\n",
    "# because the global values of the parameters are updated during the sweep!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c2a4a1",
   "metadata": {},
   "source": [
    "## Detection\n",
    "Now that we have a correlation map, we can segment the cells. The algorithm is similar to suite2p, it does the following:\n",
    "1. Choose the peak value on the correlation map, and set this pixel as your _seed cell_\n",
    "2. Identify the _active frames_ of the candidate cell by taking all frames where the activity of the cell is greater than a threshold (`activity_thresh`) or have activity above a certain percentile (`percentile`) \n",
    "3. Look at the activity of all neighboring pixels of the _seed cell_ during the _active frames_ of the seed cell. If a candidate pixel's activity is similar to the activity of the _seed cell_ (similarity above `extend_thresh`), include the pixel in the _seed cell_.\n",
    "4. Repeat steps 2-3 until you've grown the cell as much as you can\n",
    "5. Remove the cell from the correlation map\n",
    "6. Find the next largest peak of the correlation map, take this as your seed cell and repeat steps 2-6\n",
    "7. Stop when the peak value of the remaining correlation map is below a user-specified threhsold (`peak_thresh`)\n",
    "Two main improvements over Suite2p: first, this is done in 3D. Second, it is parallelized to be much faster, as it works on patches of the movie separately!\n",
    "\n",
    "**The most important variable that you *must* set is `peak_thresh`**. To do this, use the cell below to visualize the correlation map (`vmap` for short) using napari. Use the contrast sliders to find a minimum value where all spots above this value look like they might be cells. I find it useful to set the range to be very small, all pixels above the minimum are basically white. You should try to get rid of most of the obvious noise (e.g. artifacts at edges or around blood vessels, specks of single-pixel white spots, stuff outside the brain). It is not critical to exclude everything, you can be generous here and remove ROIs based on other criteria later. However, if you are too generous, you'll end up with too many pixels above the threhsold and your detection will take forever, with a lot of extra junk cells. I recommend starting a little conservative, and then push the limits. \n",
    "\n",
    "**Other useful variables**: When you have long enough recordings, 0.2 for `extend_thresh` is OK. However, if you have only a very short recording, or you find many cells that are much larger than they should be (with large, sprinkly footprints that extend way beyond the cell), or you have large cloudy blobs of noise being picked up as cells, increase `extend_thresh`. `activity_thresh` and `percentile` work together, usually it's good enough to just pick one and change it. If you have few frames, or you feel like you have low signal, it's better to set these to be lower, so you include more frames when evaluating a cell. However, if you can afford to, it's good to keep them high (`activity_thresh` around 10, `percentile` around 99.0), because then sparsely-firing cells will be picked up easier. Play around and see!\n",
    "\n",
    "**To make it faster to try parameters, you can run the detection only on a subset of the patches**. By default the movie is split into ~100 patches (I think), but if you pass `job.patch_and_detect(do_patch_idxs=(10,20,50,80))` then the detection will only run on the specified patches.\n",
    "\n",
    "**Detection always works better with more frames!** 300 frames is a very small number, so don't expect it to work perfectly on this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f9f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the correlation map\n",
    "# here you can identify the best \"peak_thresh\"\n",
    "# play with the contrast limits of vmap until only cells are visible\n",
    "# the lower contrast limit should be used as \"peak_thresh\"\n",
    "\n",
    "# as you change the contrast limit for the vmap image, the \"viewer status\" in the bottom left will print the value for you\n",
    "\n",
    "results = job.load_corr_map_results()\n",
    "mean_img = results['mean_img']\n",
    "vmap = results['vmap']\n",
    "\n",
    "v = napari.Viewer(title=\"Identify peak_thresh!\")\n",
    "v_meanimg = v.add_image(mean_img, name='mean image')\n",
    "v_vmap = v.add_image(vmap, name='vmap')\n",
    "\n",
    "def print_contrast_limits(event):\n",
    "    v.status = f\"Current peak_thresh: {v_vmap._contrast_limits[0]:.2f}\"\n",
    "    return None\n",
    "    \n",
    "_ = v_vmap.events.connect(print_contrast_limits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ec227-39a9-4644-b903-ef74a301f839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36164f7-6a20-4274-ae15-3eedaa281f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ba7c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "job.params['detection_timebin'] = 1 \n",
    "job.params['detection_time_crop'] = (None,None)\n",
    "job.params['max_pix'] = 250\n",
    "job.params['peak_thresh'] = 78\n",
    "\n",
    "job.params['activity_thresh'] = 1.0\n",
    "job.params['extend_thresh'] = 0.3\n",
    "job.params['max_iter'] = 10000 # maximum number of ROIs detected in given patch\n",
    "combined_dir = job.patch_and_detect(combined_name='combined-full') #, do_patch_idxs=(20,30,50,60,80,90))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf02e0",
   "metadata": {},
   "source": [
    "### Compute neuropil masks, extract activity and deconvolve\n",
    "For each cell, compute a donut around it excluding all other cells to use it to estimate the local neuropil activity. Then, extract the activity of the cell and the neuropil, subtract 0.7\\*neuropil activity from the ROI activity, and deconvolve using Oasis. Make sure you have set the `tau` parameter correctly for the deconvolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.compute_npil_masks(combined_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = job.extract_and_deconvolve(stats_dir = combined_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f035d",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Visualize using napari. While the \"cells\" layer is celected, left click on a cell to see its activity traces. You can right click to manually mark cells as \"non-cells\", and you can use the sliders to filter cells based on size or the correlation map peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b36b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dir = job.dirs['detection-combined-full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89991a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ui.load_outputs(combined_dir, load_traces=True)\n",
    "v,layers = ui.create_ui(outputs)\n",
    "ui.add_callbacks_to_ui(v,layers,outputs,savedir=combined_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e4ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bc25ebc",
   "metadata": {},
   "source": [
    "## Load outputs and analyze\n",
    "This is how you can access the traces for each cell, and the locations for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ui.load_outputs(combined_dir, load_traces=True)\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6392695",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cells, n_t = outputs['F'].shape\n",
    "frame_times = n.arange(n_t) / outputs['fs']\n",
    "example_cell = 1200\n",
    "plt.plot(frame_times, outputs['F'][example_cell], label='ROI Fluorescence')\n",
    "plt.plot(frame_times, outputs['Fneu'][example_cell], label='Neuropil Fluorescence')\n",
    "plt.plot(frame_times, outputs['spks'][example_cell], label='Deconvolved activity')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Activity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b527756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results of your manual curation will be saved here\n",
    "# ROIs that are marked as cells will be 1, non-cells 0\n",
    "iscell = outputs['iscell_curated_slider']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c6d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# suite2p-style list of all cells\n",
    "cell_stats = outputs['stats']\n",
    "print(cell_stats[example_cell].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed76f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_coords = [stat['coords'] for stat in cell_stats]\n",
    "cell_centers = n.array([stat['med'] for stat in cell_stats])\n",
    "vmap_shape = outputs['vmap'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e50e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a volume with the footprints of all cells and plot a max-projection of it along z\n",
    "cell_vol = ui.fill_cells_vol(cell_coords, fill_vals = n.ones(len(cell_coords)), empty=0)\n",
    "io.show_tif((cell_vol ).max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist,bins = n.histogram(cell_centers[:,0],bins = n.arange(vmap_shape[0]))\n",
    "bins = bins[:-1]\n",
    "\n",
    "plt.plot(hist, bins)\n",
    "plt.xlabel(\"# of neurons at a given depth\")\n",
    "plt.ylabel(\"Depth from surface (um)\")\n",
    "plt.yticks(n.arange(bins.max(),0,-bins.max()//6), -15*(bins.max()-n.arange(bins.max(),0,-bins.max()//6)));\n",
    "\n",
    "# from the plot, seems like the shallowest plane has lots of cells, \n",
    "# possibly because it's out of the brain and it's mostly noise...\n",
    "# it might be a good idea to exclude them in the curation \n",
    "# probably, when you have a longer recording you won't have this issue as much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac44332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5188310d",
   "metadata": {},
   "source": [
    "### Save a fancy 3D plot\n",
    "Use UCSF Chimera to open the .mrc file and visualize your cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1, v2 = ui.make_label_vols(outputs['stats'], outputs['vmap'].shape, \n",
    "            iscell =  outputs['iscell_curated_slider'], \n",
    "                  cmap='Blues', lam_max = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae28be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.save_mrc(combined_dir, 'curated_cells.mrc',v2[:,:,:,3], voxel_size=(4,4,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f514a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339cfba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f66861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66fcb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c16f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5b0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d8ee5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5f023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4a1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a7fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f1ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e33fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0c864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1b3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf1e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8a7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938038f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
